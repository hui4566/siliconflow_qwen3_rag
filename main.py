import streamlit as st
import os
from llama_index.core import SimpleDirectoryReader, Settings, VectorStoreIndex
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai_like import OpenAILike
from dotenv import load_dotenv
import tempfile
import shutil
import base64
import io
import re

# Load environment variables
load_dotenv()

def run_rag_completion(
    documents,
    query_text: str,
    embedding_model: str = "Qwen/Qwen3-Embedding-8B",
    generative_model: str = "Qwen/Qwen3-235B-A22B"
) -> str:
    """ä½¿ç”¨ç¡…åŸºæµåŠ¨æ¨¡å‹è¿è¡ŒRAGå®Œæˆä»»åŠ¡ã€‚"""
    # é…ç½®ç¡…åŸºæµåŠ¨çš„LLM
    llm = OpenAILike(
        model=generative_model,
        api_key=os.getenv("SILICONFLOW_API_KEY"),
        api_base="https://api.siliconflow.cn/v1",
        temperature=0.7,
        max_tokens=2048,
        is_chat_model=True,
        is_function_calling_model=False
    )

    # é…ç½®ç¡…åŸºæµåŠ¨çš„åµŒå…¥æ¨¡å‹
    embed_model = OpenAIEmbedding(
        model_name=embedding_model,
        api_key=os.getenv("SILICONFLOW_API_KEY"),
        api_base="https://api.siliconflow.cn/v1"
    )
    
    Settings.llm = llm
    Settings.embed_model = embed_model
    
    # åˆ›å»ºç´¢å¼•å¹¶æŸ¥è¯¢
    index = VectorStoreIndex.from_documents(documents)
    response = index.as_query_engine(similarity_top_k=5).query(query_text)
    
    return str(response)

def display_pdf_preview(pdf_file):
    """åœ¨ä¾§è¾¹æ æ˜¾ç¤ºPDFé¢„è§ˆã€‚"""
    try:
        # æ˜¾ç¤ºPDFä¿¡æ¯
        st.sidebar.subheader("PDF é¢„è§ˆ")
        
        # å°†PDFè½¬æ¢ä¸ºbase64ç”¨äºæ˜¾ç¤º
        base64_pdf = base64.b64encode(pdf_file.getvalue()).decode('utf-8')
        
        # ä½¿ç”¨HTML iframeæ˜¾ç¤ºPDF
        pdf_display = f'<iframe src="data:application/pdf;base64,{base64_pdf}" width="100%" height="500" type="application/pdf"></iframe>'
        st.sidebar.markdown(pdf_display, unsafe_allow_html=True)
        
        return True
    except Exception as e:
        st.sidebar.error(f"é¢„è§ˆPDFæ—¶å‡ºé”™: {str(e)}")
        return False

def format_reasoning_response(thinking_content):
    """æ ¼å¼åŒ–åŠ©æ‰‹å†…å®¹ï¼Œç§»é™¤æ€è€ƒæ ‡ç­¾ã€‚"""
    return (
        thinking_content.replace("<think>\n\n</think>", "")
        .replace("<think>", "")
        .replace("</think>", "")
    )

def display_assistant_message(content):
    """æ˜¾ç¤ºåŠ©æ‰‹æ¶ˆæ¯ï¼Œå¦‚æœå­˜åœ¨æ€è€ƒå†…å®¹åˆ™å±•ç¤ºã€‚"""
    pattern = r"<think>(.*?)</think>"
    think_match = re.search(pattern, content, re.DOTALL)
    if think_match:
        think_content = think_match.group(0)
        response_content = content.replace(think_content, "")
        think_content = format_reasoning_response(think_content)
        with st.expander("AI æ¨ç†è¿‡ç¨‹"):
            st.markdown(think_content)
        st.markdown(response_content)
    else:
        st.markdown(content)

def main():
    """ä¸»å‡½æ•°ï¼Œè¿è¡ŒStreamlitåº”ç”¨ç¨‹åºã€‚"""
    st.set_page_config(page_title="ç¡…åŸºæµåŠ¨ Qwen3 RAG èŠå¤©", layout="wide")
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "docs_loaded" not in st.session_state:
        st.session_state.docs_loaded = False
    if "temp_dir" not in st.session_state:
        st.session_state.temp_dir = None
    if "current_pdf" not in st.session_state:
        st.session_state.current_pdf = None
    
    # æ ‡é¢˜å’ŒæŒ‰é’®
    col1, col2 = st.columns([4, 1])
    with col1:
        # åˆ›å»ºå¸¦æœ‰å›¾æ ‡çš„æ ‡é¢˜
        title_html = """
        <div style="display: flex; align-items: center; gap: 10px;">
            <h1 style="margin: 0;">ğŸ¤– RAG Chat with Qwen3 & LlamaIndex</h1>
        </div>
        """
        st.markdown(title_html, unsafe_allow_html=True)
    with col2:
        if st.button("ğŸ—‘ï¸ æ¸…é™¤èŠå¤©"):
            st.session_state.messages = []
            st.session_state.docs_loaded = False
            if st.session_state.temp_dir:
                shutil.rmtree(st.session_state.temp_dir)
                st.session_state.temp_dir = None
            st.session_state.current_pdf = None
            st.rerun()
    
    st.caption("ç”±ç¡…åŸºæµåŠ¨ AI é©±åŠ¨")
    
    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.markdown("### ğŸš€ ç¡…åŸºæµåŠ¨ Qwen3 RAG")
        
        # æ¨¡å‹é€‰æ‹©
        generative_model = st.selectbox(
            "ç”Ÿæˆæ¨¡å‹",
            ["Qwen/Qwen3-235B-A22B", "deepseek-ai/DeepSeek-V3"],
            index=0
        )
        
        embedding_model = st.selectbox(
            "åµŒå…¥æ¨¡å‹",
            ["Qwen/Qwen3-Embedding-8B"],
            index=0
        )
        
        st.divider()
        
        # PDFæ–‡ä»¶ä¸Šä¼ 
        st.subheader("ğŸ“„ ä¸Šä¼  PDF")
        uploaded_file = st.file_uploader(
            "é€‰æ‹©ä¸€ä¸ªPDFæ–‡ä»¶",
            type="pdf",
            accept_multiple_files=False
        )
        
        # å¤„ç†PDFä¸Šä¼ å’Œå¤„ç†
        if uploaded_file is not None:
            if uploaded_file != st.session_state.current_pdf:
                st.session_state.current_pdf = uploaded_file
                try:
                    if not os.getenv("SILICONFLOW_API_KEY"):
                        st.error("ç¼ºå°‘ç¡…åŸºæµåŠ¨ API å¯†é’¥")
                        st.stop()
                    
                    # ä¸ºPDFåˆ›å»ºä¸´æ—¶ç›®å½•
                    if st.session_state.temp_dir:
                        shutil.rmtree(st.session_state.temp_dir)
                    st.session_state.temp_dir = tempfile.mkdtemp()
                    
                    # å°†ä¸Šä¼ çš„PDFä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
                    file_path = os.path.join(st.session_state.temp_dir, uploaded_file.name)
                    with open(file_path, "wb") as f:
                        f.write(uploaded_file.getbuffer())
                    
                    with st.spinner("æ­£åœ¨åŠ è½½PDF..."):
                        # ä»ä¸´æ—¶ç›®å½•åŠ è½½æ–‡æ¡£
                        documents = SimpleDirectoryReader(st.session_state.temp_dir).load_data()
                        st.session_state.docs_loaded = True
                        st.session_state.documents = documents
                        st.success("âœ“ PDF åŠ è½½æˆåŠŸ")
                        
                        # æ˜¾ç¤ºPDFé¢„è§ˆ
                        display_pdf_preview(uploaded_file)
                except Exception as e:
                    st.error(f"é”™è¯¯: {str(e)}")
    
    # æ˜¾ç¤ºèŠå¤©æ¶ˆæ¯
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if message["role"] == "assistant":
                display_assistant_message(message["content"])
            else:
                st.markdown(message["content"])
    
    # èŠå¤©è¾“å…¥
    if prompt := st.chat_input("è¯¢é—®æ‚¨çš„PDFå†…å®¹..."):
        if not st.session_state.docs_loaded:
            st.error("è¯·å…ˆä¸Šä¼ PDFæ–‡ä»¶")
            st.stop()
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # ç”Ÿæˆå“åº”
        with st.chat_message("assistant"):
            with st.spinner("æ­£åœ¨æ€è€ƒ..."):
                try:
                    response = run_rag_completion(
                        st.session_state.documents,
                        prompt,
                        embedding_model,
                        generative_model
                    )
                    st.session_state.messages.append({"role": "assistant", "content": response})
                    display_assistant_message(response)
                except Exception as e:
                    st.error(f"é”™è¯¯: {str(e)}")

if __name__ == "__main__":
    main()
